{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba3096d6",
   "metadata": {},
   "source": [
    "# Running Models Locally with Docker Model Runner\n",
    "\n",
    "Docker Model Runner (DMR) is a Docker Desktop feature that enables the run of Large Language Models natively with Docker Desktop. This feature follows the common Docker workflow:\n",
    "\n",
    "- Pull models from registries (e.g., Docker Hub)\n",
    "- Run models locally with GPU acceleration\n",
    "- Integrate models into the development workflows\n",
    "\n",
    "Of course, the LLMs' performance is derived from the model size and resources available locally.\n",
    "\n",
    ">This feature is currently under Beta, and required Docker Engine (Linux) or Docker Desktop 4.40 and above for MacOS, and Docker Desktop 4.41 for Windows Docker Engine. For hardware requirements, please check the Docker Model Runner documentation\n",
    "\n",
    "DMR runs as a standalone server so that you can connect to it from both containerized environments and regular local Python environments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7578a7",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"assets/docker model runner.png\" width=\"85%\" align=\"center\"/></a>\n",
    "<figcaption> Docker Model Runner Server </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e598eb9",
   "metadata": {},
   "source": [
    "DMR is compatible with the OpenAI API SDKs. This makes it easy to adapt existing code that uses the OpenAI API to work with DMR and interact with locally running LLMs. In this tutorial, we'll focus on the Python SDK, though the same approach applies to other OpenAI SDKs like JavaScript, Java, Go, .NET, and more.\n",
    "\n",
    "As we demo before with the Google Gemini and Anthropic Cloud APIs, we will need  first to set the client by defining the base URL and API key.\n",
    "\n",
    "By default, the DRM uses `docker` as the API key, and the base URL is set based off the calling methods:\n",
    "- Local virtual environment\n",
    "- Containerized environment\n",
    "\n",
    "In the following example, we will download a Llama 3.2 model from Docker Hub, and use the OpenAI API SDK to interact with the model locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665fad97",
   "metadata": {},
   "source": [
    "## Download LLM from Docker Hub\n",
    "\n",
    "We will download the model from Docker Hub from the CLI using the `docker model pull` command. \n",
    "\n",
    "First, make sure that you are logged in to Docker Hub:\n",
    "\n",
    "```shell\n",
    "docker login\n",
    "```\n",
    "\n",
    "Next, we will confirm that DMR is active by using the `docker model status` command:\n",
    "\n",
    "```shell\n",
    "docker model status\n",
    "```\n",
    "\n",
    "The following output indicated that DMR is active and ready to use:\n",
    "``` shell\n",
    "Docker Model Runner is running\n",
    "\n",
    "Status:\n",
    "llama.cpp: running llama.cpp latest-metal (sha256:41df5190b7121f6509a278b8af657732f42b3715155893b5993ed4b28c53b92d) version: 82bf586\n",
    "```\n",
    "\n",
    "Next, we will pull the selected model from Docker Hub:\n",
    "\n",
    "```shell\n",
    "docker model pull ai/llama3.2:3B-Q4_0\n",
    "```\n",
    "\n",
    "You should expect the following output:\n",
    "\n",
    "``` shell\n",
    "Downloaded: 0.00 MB\n",
    "Model pulled successfully\n",
    "```\n",
    "\n",
    "You can use the `docker model list` to see the model details:\n",
    "\n",
    "``` shell\n",
    "docker model list\n",
    "```\n",
    "\n",
    "This returns the following:\n",
    "``` shell\n",
    "MODEL NAME                                   PARAMETERS  QUANTIZATION    ARCHITECTURE  MODEL ID      CREATED        SIZE\n",
    "ai/llama3.2:latest                           3.21 B      IQ2_XXS/Q4_K_M  llama         436bb282b419  5 months ago   1.87 GiB\n",
    "```\n",
    "\n",
    "> Note that DMR follows the Open Container Initiative (OCI) standards for model registry and supports the [GGUF](https://huggingface.co/docs/hub/en/gguf) file format for packaging models as OCI Artifacts. Therefore, you can download models that follow this format from Hugging Face.\n",
    "\n",
    "Once the model was downloaded, we can start using it with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0584ef",
   "metadata": {},
   "source": [
    "## Using the OpenAI API Python SDK\n",
    "\n",
    "Let's pivot to Python, and follow the same workflow as we did with the OpenAI, Google Gemini, and Anthropic Claude APIs using the OpenAI API Python SDK. \n",
    "\n",
    "Let's start by loading the `openai` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb834b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5922ff70",
   "metadata": {},
   "source": [
    "Next, we will set the client object. The base URL setting depends on whether you run your code in a containerized or local environment.\n",
    "\n",
    "If you are running your code locally with virtual environment (i.e., not inside a container), you should enable the TCP and set a port by running the following from the CLI:\n",
    "\n",
    "```shell\n",
    "docker desktop enable model-runner --tcp=12434\n",
    "```\n",
    "And than use the following URL:\n",
    "```\n",
    "http://localhost:12434/engines/v1\n",
    "```\n",
    "\n",
    "Where the `12434` represents the port number that you have exposed.\n",
    "\n",
    "Otherwise, if are using a containerized environment, you should use the following URL:\n",
    "\n",
    "```\n",
    "http://model-runner.docker.internal/engines/v1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7047fb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case using a containerized environment:\n",
    "base_url = \"http://model-runner.docker.internal/engines/v1\"\n",
    "\n",
    "# Case running outside a container uncomment the code\n",
    "# base_url = \"http://localhost:12434/engines/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f023e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url=base_url, api_key=\"docker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f00f596",
   "metadata": {},
   "source": [
    "We will use the same prompt as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e143fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_prompt = \"\"\"\n",
    "I am working with a dataset that contains information about the Red30 company's product sales online.\n",
    "I want to create a SQL query that calculates the total sales by state.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f396c52d",
   "metadata": {},
   "source": [
    "Let's set the temperature and max tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a2ae242",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0\n",
    "max_tokens = 5000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1d166b",
   "metadata": {},
   "source": [
    "Next, we will set the chat completions method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5da5073",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_llama = client.chat.completions.create(\n",
    "    model=\"ai/llama3.2:latest\",\n",
    "    messages=[{\"role\": \"user\", \"content\": content_prompt}],\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ad1c8",
   "metadata": {},
   "source": [
    "Let's parse the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e5f7cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an example SQL query that calculates the total sales by state for the Red30 company's product sales online.\n",
      "\n",
      "```sql\n",
      "-- Create a table for the data\n",
      "CREATE TABLE sales_data (\n",
      "    id INT PRIMARY KEY,\n",
      "    state VARCHAR(255),\n",
      "    product VARCHAR(255),\n",
      "    sales DECIMAL(10, 2)\n",
      ");\n",
      "\n",
      "-- Insert sample data\n",
      "INSERT INTO sales_data (id, state, product, sales)\n",
      "VALUES\n",
      "(1, 'California', 'Product A', 1000.00),\n",
      "(2, 'California', 'Product B', 500.00),\n",
      "(3, 'New York', 'Product A', 2000.00),\n",
      "(4, 'New York', 'Product B', 1500.00),\n",
      "(5, 'Florida', 'Product A', 3000.00),\n",
      "(6, 'Florida', 'Product B', 2500.00);\n",
      "\n",
      "-- SQL query to calculate total sales by state\n",
      "SELECT \n",
      "    state,\n",
      "    SUM(sales) AS total_sales\n",
      "FROM \n",
      "    sales_data\n",
      "GROUP BY \n",
      "    state;\n",
      "```\n",
      "\n",
      "This query will return a result set with the state and the total sales for each state.\n",
      "\n",
      "Example output:\n",
      "\n",
      "| state    | total_sales |\n",
      "|----------|-------------|\n",
      "| California| 2500.00     |\n",
      "| New York  | 3500.00     |\n",
      "| Florida   | 5500.00     |\n",
      "\n",
      "This query uses the `GROUP BY` clause to group the data by state, and the `SUM` function to calculate the total sales for each state.\n",
      "\n",
      "If you want to include states with zero sales, you can use the `LEFT JOIN` or `FULL OUTER JOIN` clause to include them in the result set.\n",
      "\n",
      "```sql\n",
      "SELECT \n",
      "    s.state,\n",
      "    COALESCE(sd.total_sales, 0) AS total_sales\n",
      "FROM \n",
      "    sales_data sd\n",
      "LEFT JOIN \n",
      "    (SELECT state, SUM(sales) AS total_sales\n",
      "     FROM sales_data\n",
      "     GROUP BY state) s ON sd.state = s.state;\n",
      "```\n",
      "\n",
      "This query will return a result set with all states, including those with zero sales.\n",
      "\n",
      "Example output:\n",
      "\n",
      "| state    | total_sales |\n",
      "|----------|-------------|\n",
      "| California| 2500.00     |\n",
      "| New York  | 3500.00     |\n",
      "| Florida   | 5500.00     |\n",
      "| Texas     | 0.00        |\n",
      "| Other     | 0.00        |\n"
     ]
    }
   ],
   "source": [
    "print(response_llama.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.11-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
