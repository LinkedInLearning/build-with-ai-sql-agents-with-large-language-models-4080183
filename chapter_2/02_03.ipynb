{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba3096d6",
   "metadata": {},
   "source": [
    "# Running Models Locally with Docker Model Runner\n",
    "\n",
    "Docker Model Runner (DMR) is a Docker Desktop feature that enables the run of Large Language Models natively with Docker Desktop. This feature follows the common Docker workflow:\n",
    "\n",
    "- Pull models from registries (e.g., Docker Hub)\n",
    "- Run models locally with GPU acceleration\n",
    "- Integrate models into the development workflows\n",
    "\n",
    "Of course, the LLMs' performance is derived from the model size and resources available locally.\n",
    "\n",
    ">This feature is currently under Beta, and required Docker Engine (Linux) or Docker Desktop 4.40 and above for MacOS, and Docker Desktop 4.41 for Windows Docker Engine. For hardware requirements, please check the Docker Model Runner documentation\n",
    "\n",
    "DMR runs as a standalone server so that you can connect to it from both containerized environments and regular local Python environments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7578a7",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"assets/dmr diagram.png\" width=\"100%\" align=\"center\"/></a>\n",
    "<figcaption> Docker Model Runner Server </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e598eb9",
   "metadata": {},
   "source": [
    "DMR is compatible with the OpenAI API SDKs. This makes it easy to adapt existing code that uses the OpenAI API to work with DMR and interact with locally running LLMs. In this tutorial, we'll focus on the Python SDK, though the same approach applies to other OpenAI SDKs like JavaScript, Java, Go, .NET, and more.\n",
    "\n",
    "As we demo before with the Google Gemini and Anthropic Cloud APIs, we will need  first to set the client by defining the base URL and API key.\n",
    "\n",
    "By default, the DRM uses `docker` as the API key, and the base URL is set based off the calling methods:\n",
    "- Local virtual environment\n",
    "- Containerized environment\n",
    "\n",
    "In the following example, we will download a Llama 3.2 model from Docker Hub, and use the OpenAI API SDK to interact with the model locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665fad97",
   "metadata": {},
   "source": [
    "## Download LLM from Docker Hub\n",
    "\n",
    "We will download the model from Docker Hub from the CLI using the `docker model pull` command. \n",
    "\n",
    "First, make sure that you are logged in to Docker Hub:\n",
    "\n",
    "```shell\n",
    "docker login\n",
    "```\n",
    "\n",
    "Next, we will confirm that DMR is active by using the `docker model status` command:\n",
    "\n",
    "```shell\n",
    "docker model status\n",
    "```\n",
    "\n",
    "The following output indicated that DMR is active and ready to use:\n",
    "``` shell\n",
    "Docker Model Runner is running\n",
    "\n",
    "Status:\n",
    "llama.cpp: running llama.cpp latest-metal (sha256:41df5190b7121f6509a278b8af657732f42b3715155893b5993ed4b28c53b92d) version: 82bf586\n",
    "```\n",
    "\n",
    "Next, we will pull the selected model from Docker Hub:\n",
    "\n",
    "```shell\n",
    "docker model pull ai/llama3.2:3B-Q4_0\n",
    "```\n",
    "\n",
    "You should expect the following output:\n",
    "\n",
    "``` shell\n",
    "Downloaded: 0.00 MB\n",
    "Model pulled successfully\n",
    "```\n",
    "\n",
    "You can use the `docker model list` to see the model details:\n",
    "\n",
    "``` shell\n",
    "docker model list\n",
    "```\n",
    "\n",
    "This returns the following:\n",
    "``` shell\n",
    "MODEL NAME                                   PARAMETERS  QUANTIZATION    ARCHITECTURE  MODEL ID      CREATED        SIZE\n",
    "ai/llama3.2:latest                           3.21 B      IQ2_XXS/Q4_K_M  llama         436bb282b419  5 months ago   1.87 GiB\n",
    "```\n",
    "\n",
    "> Note that DMR follows the Open Container Initiative (OCI) standards for model registry and supports the [GGUF](https://huggingface.co/docs/hub/en/gguf) file format for packaging models as OCI Artifacts. Therefore, you can download models that follow this format from Hugging Face.\n",
    "\n",
    "Once the model was downloaded, we can start using it with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0584ef",
   "metadata": {},
   "source": [
    "## Using the OpenAI API Python SDK\n",
    "\n",
    "Let's pivot to Python, and follow the same workflow as we did with the OpenAI, Google Gemini, and Anthropic Claude APIs using the OpenAI API Python SDK. \n",
    "\n",
    "Let's start by loading the `openai` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb834b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5922ff70",
   "metadata": {},
   "source": [
    "Next, we will set the client object. The base URL setting depends on whether you run your code in a containerized or local environment.\n",
    "\n",
    "If you are running your code locally with virtual environment (i.e., not inside a container), you should enable the TCP and set a port by running the following from the CLI:\n",
    "\n",
    "```shell\n",
    "docker desktop enable model-runner --tcp=12434\n",
    "```\n",
    "And than use the following URL:\n",
    "```\n",
    "http://localhost:12434/engines/v1\n",
    "```\n",
    "\n",
    "Where the `12434` represents the port number that you have exposed.\n",
    "\n",
    "Otherwise, if are using a containerized environment, you should use the following URL:\n",
    "\n",
    "```\n",
    "http://model-runner.docker.internal/engines/v1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7047fb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case using a containerized environment:\n",
    "base_url = \"http://model-runner.docker.internal/engines/v1\"\n",
    "\n",
    "# Case running outside a container uncomment the code\n",
    "# base_url = \"http://localhost:12434/engines/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fad3c",
   "metadata": {},
   "source": [
    "Next, we will set the client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0652992",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(base_url=base_url, api_key=\"docker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f00f596",
   "metadata": {},
   "source": [
    "We will use the same prompt as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e143fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_prompt = \"\"\"\n",
    "I am working with a dataset that contains information about Chicago crime incidents. \n",
    "I want to create a SQL query to pull the total number of crimes that ended in arrest by a year\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f396c52d",
   "metadata": {},
   "source": [
    "Let's set the temperature and max tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a2ae242",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0\n",
    "max_tokens = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5da5073",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_llama = client.chat.completions.create(\n",
    "    model=\"ai/llama3.2:latest\",\n",
    "    messages=[{\"role\": \"user\", \"content\": content_prompt}],\n",
    "    temperature=temperature,\n",
    "    max_tokens=max_tokens,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ad1c8",
   "metadata": {},
   "source": [
    "Let's parse the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e5f7cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a SQL query that should accomplish what you're looking for:\n",
      "\n",
      "```sql\n",
      "SELECT \n",
      "    YEAR(date) AS year,\n",
      "    COUNT(CASE WHEN outcome = 'Arrest' THEN 1 END) AS total_arrests\n",
      "FROM \n",
      "    crimes\n",
      "GROUP BY \n",
      "    YEAR(date)\n",
      "ORDER BY \n",
      "    year;\n",
      "```\n",
      "\n",
      "This query works as follows:\n",
      "\n",
      "- It selects the year from the `date` column and counts the number of rows where the `outcome` is 'Arrest'. This is done using a `CASE` statement within the `COUNT` function.\n",
      "- The `GROUP BY` clause groups the results by year.\n",
      "- Finally, the results are ordered by year.\n",
      "\n",
      "Please note that this assumes that the `date` column is of a date type and that the `outcome` column is of a string type. If your column types are different, you may need to adjust the query accordingly.\n",
      "\n",
      "Also, make sure that the `date` column is in a format that can be used by the `YEAR` function. If it's in a different format, you may need to convert it first.\n",
      "\n",
      "Example use case:\n",
      "\n",
      "Let's say you have a table called `crimes` with the following data:\n",
      "\n",
      "| date       | outcome |\n",
      "|------------|---------|\n",
      "| 2020-01-01 | Arrest  |\n",
      "| 2020-02-01 | No Arrest|\n",
      "| 2020-03-01 | Arrest  |\n",
      "| 2021-01-01 | Arrest  |\n",
      "| 2021-02-01 | No Arrest|\n",
      "| 2021-03-01 | Arrest  |\n",
      "\n",
      "Running the above query on this data would produce the following result:\n",
      "\n",
      "| year | total_arrests |\n",
      "|------|----------------|\n",
      "| 2020 | 2              |\n",
      "| 2021 | 2              |\n",
      "\n",
      "This shows that in 2020, there were 2 crimes that ended in arrest, and in 2021, there were also 2 crimes that ended in arrest.\n"
     ]
    }
   ],
   "source": [
    "print(response_llama.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.11-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
